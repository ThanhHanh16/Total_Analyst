{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0c5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ƒê·ªçc file g·ªëc\n",
    "excel_path = \"data_cleaned.xlsx\"\n",
    "df = pd.read_excel(excel_path, sheet_name= 'customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb147dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DOB'] = pd.to_datetime(df['DOB'], origin='1899-12-30', unit='D', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddfc7e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DOB\n",
      "0 1999-04-18\n",
      "1 1998-04-16\n",
      "2 1993-12-16\n",
      "3 1999-11-08\n",
      "4 1994-10-01\n"
     ]
    }
   ],
   "source": [
    "print(df[['DOB']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe539f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerid          0\n",
       "DOB                 0\n",
       "gender              0\n",
       "address             1\n",
       "job                 0\n",
       "industry         1122\n",
       "year_of_birth       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = ['gender', 'address','website', 'job', 'industry']\n",
    "for col in string_columns:\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['Website'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda92ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_of_birth'] = df['DOB'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88897c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T√¨m nƒÉm sinh ph·ªï bi·∫øn nh·∫•t theo job\n",
    "most_common_year_by_job = {}\n",
    "for job in df['job'].dropna().unique():\n",
    "    subset = df[(df['job'] == job) & (df['year_of_birth'].notna())]['year_of_birth']\n",
    "    if not subset.empty:\n",
    "        mode_year = subset.mode()\n",
    "        if not mode_year.empty:\n",
    "            most_common_year_by_job[job] = int(mode_year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3d2e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#H√†m ƒëi·ªÅn DOB b·ªã thi·∫øu\n",
    "def fill_missing_dob(row):\n",
    "    if pd.isna(row['DOB']):\n",
    "        job = row['job']\n",
    "        if job in most_common_year_by_job:\n",
    "            year = most_common_year_by_job[job]\n",
    "            return pd.Timestamp(f'{year}-01-01')\n",
    "    return row['DOB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b72f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. √Åp d·ª•ng h√†m ƒë·ªÉ ƒëi·ªÅn DOB\n",
    "df['DOB'] = df.apply(fill_missing_dob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a16d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. C·∫≠p nh·∫≠t l·∫°i year_of_birth sau khi ƒë√£ ƒëi·ªÅn th√™m DOB\n",
    "df['year_of_birth'] = df['DOB'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6beb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerid       0\n",
       "DOB              0\n",
       "gender           0\n",
       "address          0\n",
       "job              0\n",
       "industry         0\n",
       "year_of_birth    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cae6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "film = pd.read_excel(excel_path, sheet_name = \"film\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e4e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_id          0\n",
       "title            0\n",
       "director        10\n",
       "cast             6\n",
       "country          7\n",
       "release_year     0\n",
       "rating           1\n",
       "duration         0\n",
       "listed_in        0\n",
       "description      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1e41b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l√†m s·∫°ch d·ªØ li·ªáu c·ªôt desciption c√† cast c·ªßa film\n",
    "from ftfy import fix_text\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = fix_text(text)  # S·ª≠a l·ªói m√£ h√≥a\n",
    "        for ch in ['‚Äú', '‚Äù', '\"', \"'\"]:\n",
    "            text = text.replace(ch, '')  # B·ªè d·∫•u ngo·∫∑c k√©p\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# √Åp d·ª•ng cho c√°c c·ªôt\n",
    "film['description'] = film['description'].apply(clean_text)\n",
    "film['cast'] = film['cast'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee09c1",
   "metadata": {},
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c198e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cast</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James McAvoy, Michael Fassbender, Jennifer Law...</td>\n",
       "      <td>When Jean Grey transforms into the Dark Phoeni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louis Ashbourne Serkis, Tom Taylor, Rebecca Fe...</td>\n",
       "      <td>When a kid discovers the legendary sword, Exca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a-chan , KASHIYUKA , NOCCHi</td>\n",
       "      <td>J-Pop band Perfume shares their passion for mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOSHIKI</td>\n",
       "      <td>Yoshiki from X Japan performs two Disney songs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dan Nachtrab</td>\n",
       "      <td>Great Shark Chow Down ‚Äì prepare for a feast of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jeremiah Sullivan, Dave Hoffman</td>\n",
       "      <td>Marine biologist attempts to get bitten by a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bert Morris</td>\n",
       "      <td>Discover the technical prowess behind Petra, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jay Sanders</td>\n",
       "      <td>National Geographic reconstructs the Ulfberht,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Maite Jauregui</td>\n",
       "      <td>Forensic experts scan Pompeiis victims to inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lo√Øck Peyron</td>\n",
       "      <td>Lo√Øck Peyron investigates the 1978 Amoco Cadiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Colin Solman</td>\n",
       "      <td>As we follow the robbers life cycle, we learn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Paul Bandey</td>\n",
       "      <td>On April 15, 2019, Paris firefighters gave eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Neil Harris, Kaley Cuoco, Oscar Isaac, Sarah H...</td>\n",
       "      <td>Celebrating Disneys new land that brings Star ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Craig Sechler</td>\n",
       "      <td>Who were the mysterious people who built Machu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stefan Frank</td>\n",
       "      <td>Ancient civilizations shared an incredible kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Julianna Margulies</td>\n",
       "      <td>How will you make the world a better place? He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Angelina Jolie, Elle Fanning, Chiwetel Ejiofor...</td>\n",
       "      <td>The story of Disneys most iconic villain conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Carrie Fisher, Mark Hamill, Adam Driver, Daisy...</td>\n",
       "      <td>The landmark Skywalker saga comes to a conclus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ed Helms</td>\n",
       "      <td>Steve the penguin embarks on an epic quest to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kristen Bell, Idina Menzel, Josh Gad, Jonathan...</td>\n",
       "      <td>Elsa journeys into the unknown to uncover trut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 cast  \\\n",
       "0   James McAvoy, Michael Fassbender, Jennifer Law...   \n",
       "1   Louis Ashbourne Serkis, Tom Taylor, Rebecca Fe...   \n",
       "2                         a-chan , KASHIYUKA , NOCCHi   \n",
       "3                                             YOSHIKI   \n",
       "4                                        Dan Nachtrab   \n",
       "5                     Jeremiah Sullivan, Dave Hoffman   \n",
       "6                                         Bert Morris   \n",
       "7                                         Jay Sanders   \n",
       "8                                      Maite Jauregui   \n",
       "9                                        Lo√Øck Peyron   \n",
       "10                                       Colin Solman   \n",
       "11                                        Paul Bandey   \n",
       "12  Neil Harris, Kaley Cuoco, Oscar Isaac, Sarah H...   \n",
       "13                                      Craig Sechler   \n",
       "14                                       Stefan Frank   \n",
       "15                                 Julianna Margulies   \n",
       "16  Angelina Jolie, Elle Fanning, Chiwetel Ejiofor...   \n",
       "17  Carrie Fisher, Mark Hamill, Adam Driver, Daisy...   \n",
       "18                                           Ed Helms   \n",
       "19  Kristen Bell, Idina Menzel, Josh Gad, Jonathan...   \n",
       "\n",
       "                                          description  \n",
       "0   When Jean Grey transforms into the Dark Phoeni...  \n",
       "1   When a kid discovers the legendary sword, Exca...  \n",
       "2   J-Pop band Perfume shares their passion for mu...  \n",
       "3   Yoshiki from X Japan performs two Disney songs...  \n",
       "4   Great Shark Chow Down ‚Äì prepare for a feast of...  \n",
       "5   Marine biologist attempts to get bitten by a d...  \n",
       "6   Discover the technical prowess behind Petra, a...  \n",
       "7   National Geographic reconstructs the Ulfberht,...  \n",
       "8   Forensic experts scan Pompeiis victims to inve...  \n",
       "9   Lo√Øck Peyron investigates the 1978 Amoco Cadiz...  \n",
       "10  As we follow the robbers life cycle, we learn ...  \n",
       "11  On April 15, 2019, Paris firefighters gave eve...  \n",
       "12  Celebrating Disneys new land that brings Star ...  \n",
       "13  Who were the mysterious people who built Machu...  \n",
       "14  Ancient civilizations shared an incredible kno...  \n",
       "15  How will you make the world a better place? He...  \n",
       "16  The story of Disneys most iconic villain conti...  \n",
       "17  The landmark Skywalker saga comes to a conclus...  \n",
       "18  Steve the penguin embarks on an epic quest to ...  \n",
       "19  Elsa journeys into the unknown to uncover trut...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film[['cast', 'description']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\data _cinema\\notebook\\data_cleaned.xlsx\"\n",
    "readme = pd.read_excel(excel_path, sheet_name = \"readme\")\n",
    "ticket = pd.read_excel(excel_path, sheet_name = \"ticket\")\n",
    "\n",
    "# ƒê·ªçc l·∫°i to√†n b·ªô sheet\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    readme.to_excel(writer, sheet_name='readme', index=False)\n",
    "    df.to_excel(writer, sheet_name='customer', index=False)\n",
    "    ticket.to_excel(writer, sheet_name='ticket', index=False)\n",
    "    film.to_excel(writer, sheet_name='film', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d29025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ƒë√£ ghi ƒë√® c·ªôt 'address' trong sheet 'customer'.\n",
      "                                           address\n",
      "0                P. H√≤a Kh√™, Q. Thanh Kh√™, ƒê√† N·∫µng\n",
      "1                 P. M√¢n Th√°i, Q. S∆°n Tr√†, ƒê√† N·∫µng\n",
      "2                            H. Qu·∫ø S∆°n, Qu·∫£ng Nam\n",
      "3           ƒê∆∞·ªùng Tr∆∞·ªùng Chinh, Q. C·∫©m L·ªá, ƒê√† N·∫µng\n",
      "4                               Q. C·∫©m L·ªá, ƒê√† N·∫µng\n",
      "...                                            ...\n",
      "4474                                       ƒê√† N·∫µng\n",
      "4475              P. Ph∆∞·ªõc M·ªπ, Q. S∆°n Tr√†, ƒê√† N·∫µng\n",
      "4476                             Nguyen Gian Thanh\n",
      "4477  ƒê∆∞·ªùng Ng≈© H√†nh S∆°n, Q. Ng≈© H√†nh S∆°n, ƒê√† N·∫µng\n",
      "4478       P. Thanh Kh√™ T√¢y, Q. Thanh Kh√™, ƒê√† N·∫µng\n",
      "\n",
      "[4479 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re, unicodedata, difflib, pandas as pd\n",
    "import shutil\n",
    "\n",
    "# ====== C·∫§U H√åNH ======\n",
    "excel_path = \"data_cleaned.xlsx\"\n",
    "src_sheet  = \"customer\"          # sheet ch·ª©a c·ªôt address\n",
    "addr_col   = \"address\"           # t√™n c·ªôt ƒë·ªãa ch·ªâ\n",
    "DEFAULT_CITY = \"ƒê√† N·∫µng\"\n",
    "OVERWRITE_BACKUP = True          # backup file g·ªëc\n",
    "\n",
    "if OVERWRITE_BACKUP:\n",
    "    shutil.copyfile(excel_path, excel_path + \".bak\")\n",
    "\n",
    "# ==== 0) Utils ====\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize('NFKD', str(s))\n",
    "    s = ''.join(c for c in s if not unicodedata.combining(c))\n",
    "    return s.replace('ƒê','D').replace('ƒë','d')\n",
    "\n",
    "def norm_key(s: str) -> str:\n",
    "    s = strip_accents(str(s)).lower()\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def best_match(s, candidates, cutoff=0.86):\n",
    "    if not s: return (None, 0.0)\n",
    "    key = norm_key(s)\n",
    "    keys = list(candidates.keys())\n",
    "    matches = difflib.get_close_matches(key, keys, n=1, cutoff=cutoff)\n",
    "    if matches:\n",
    "        mk = matches[0]\n",
    "        score = difflib.SequenceMatcher(None, key, mk).ratio()\n",
    "        return (candidates[mk], score)\n",
    "    return (None, 0.0)\n",
    "\n",
    "def clean_street_name(name: str) -> str:\n",
    "    # lo·∫°i ti·ªÅn t·ªë ƒê∆∞·ªùng/Duong/ƒêg...\n",
    "    return re.sub(r'^(ƒë∆∞·ªùng|duong|ƒëg\\.?|dg\\.?)\\s+', '', str(name), flags=re.I).strip()\n",
    "\n",
    "# ==== 1) ƒêI·ªÄN TH·ª¶ C√îNG T·∫†I ƒê√ÇY ====\n",
    "# üëâ B·∫°n t·ª± th√™m/s·ª≠a danh s√°ch ph∆∞·ªùng & t√™n ƒë∆∞·ªùng cho t·ª´ng qu·∫≠n.\n",
    "#    M·∫´u ƒë√∫ng: m·ªói qu·∫≠n l√† 1 key \"Q. T√™n Qu·∫≠n\": [ \"T√™n 1\", \"T√™n 2\", ... ]\n",
    "\n",
    "WARDS_BY_DISTRICT = {\n",
    "    \"Q. H·∫£i Ch√¢u\":   [\"Thu·∫≠n Ph∆∞·ªõc\",\"Th·∫°ch Thang\",\"Thanh B√¨nh\",\"H·∫£i Ch√¢u I\",\"H·∫£i Ch√¢u II\",\n",
    "        \"Ph∆∞·ªõc Ninh\",\"H√≤a Thu·∫≠n T√¢y\",\"H√≤a Thu·∫≠n ƒê√¥ng\",\"Nam D∆∞∆°ng\",\n",
    "        \"B√¨nh Hi√™n\",\"B√¨nh Thu·∫≠n\",\"H√≤a C∆∞·ªùng B·∫Øc\",\"H√≤a C∆∞·ªùng Nam\"],        # v√≠ d·ª•\n",
    "    \"Q. Thanh Kh√™\":  [\"An Kh√™\",\"Ch√≠nh Gi√°n\",\"H√≤a Kh√™\",\"Tam Thu·∫≠n\",\"T√¢n Ch√≠nh\",\n",
    "        \"Th·∫°c Gi√°n\",\"Thanh Kh√™ ƒê√¥ng\",\"Thanh Kh√™ T√¢y\",\"Vƒ©nh Trung\",\"Xu√¢n H√†\"],\n",
    "    \"Q. S∆°n Tr√†\":    [\"An H·∫£i B·∫Øc\",\"An H·∫£i ƒê√¥ng\",\"An H·∫£i T√¢y\",\"M√¢n Th√°i\",\n",
    "                        \"N·∫°i Hi√™n ƒê√¥ng\",\"Ph∆∞·ªõc M·ªπ\",\"Th·ªç Quang\"],\n",
    "    \"Q. Ng≈© H√†nh S∆°n\": [\"H√≤a H·∫£i\",\"H√≤a Qu√Ω\",\"Khu√™ M·ªπ\",\"M·ªπ An\"],\n",
    "    \"Q. Li√™n Chi·ªÉu\": [\"H√≤a Hi·ªáp B·∫Øc\",\"H√≤a Hi·ªáp Nam\",\"H√≤a Kh√°nh B·∫Øc\",\"H√≤a Kh√°nh Nam\",\"H√≤a Minh\", \"H√≤a Kh√°nh\"],\n",
    "    \"Q. C·∫©m L·ªá\":     [\"H√≤a An\",\"H√≤a Ph√°t\",\"H√≤a Th·ªç ƒê√¥ng\",\"H√≤a Th·ªç T√¢y\",\"H√≤a Xu√¢n\",\"Khu√™ Trung\"],\n",
    "}\n",
    "\n",
    "STREETS_BY_DISTRICT = {\n",
    "    \"Q. H·∫£i Ch√¢u\": [\n",
    "        \"L√™ Du·∫©n\",\"Quang Trung\",\"Phan ƒê√¨nh Ph√πng\",\"Tr·∫ßn Ph√∫\",\"√îng √çch Khi√™m\",\n",
    "        \"Phan Ch√¢u Trinh\",\"L√™ ƒê√¨nh D∆∞∆°ng\",\"Y√™n B√°i\",\"Nguy·ªÖn Ch√≠ Thanh\",\n",
    "        \"H·∫£i Ph√≤ng\",\"H√πng V∆∞∆°ng\",\"L√Ω T·ª± Tr·ªçng\",\"Th√°i Phi√™n\",\"Pasteur\",\n",
    "        \"Ph·∫°m H·ªìng Th√°i\",\"Tri·ªáu N·ªØ V∆∞∆°ng\",\"B·∫°ch ƒê·∫±ng\",\"Duy T√¢n\",\"2 Th√°ng 9\",\n",
    "        \"Ph·∫°m Ng≈© L√£o\",\"Thanh S∆°n\", \"L√™ Du·∫©n\",\"Quang Trung\",\"Phan ƒê√¨nh Ph√πng\",\"Tr·∫ßn Ph√∫\",\"√îng √çch Khi√™m\",\n",
    "        \"Phan Ch√¢u Trinh\",\"L√™ ƒê√¨nh D∆∞∆°ng\",\"Y√™n B√°i\",\"Nguy·ªÖn Ch√≠ Thanh\",\n",
    "        \"H·∫£i Ph√≤ng\",\"H√πng V∆∞∆°ng\",\"L√Ω T·ª± Tr·ªçng\",\"Th√°i Phi√™n\",\"Pasteur\",\n",
    "        \"Ph·∫°m H·ªìng Th√°i\",\"Tri·ªáu N·ªØ V∆∞∆°ng\",\"B·∫°ch ƒê·∫±ng\",\"Duy T√¢n\",\"2 Th√°ng 9\",\n",
    "        \"Ph·∫°m Ng≈© L√£o\",\"Thanh S∆°n\", \"Nguy·ªÖn VƒÉn Linh\",\"Ho√†ng Di·ªáu\",\"Phan B·ªôi Ch√¢u\",\"Tr·∫ßn Qu·ªëc To·∫£n\",\"L√™ L·ª£i\",\"Nguy·ªÖn Du\",\"ƒê·ªëng ƒêa\",\n",
    "        \"Chu VƒÉn An\",\"Hu·ª≥nh Th√∫c Kh√°ng\",\"Ng√¥ Gia T·ª±\",\"L√Ω Th∆∞·ªùng Ki·ªát\",\"Nguy·ªÖn Tr√£i\",\n",
    "        \"H·ªì T√πng M·∫≠u\",\"Nguy·ªÖn Th√†nh H√£n\",\"C√¥ Giang\",\"Ph·∫°m ƒê√¨nh H·ªï\",\"Y√™n B√°i\",\n",
    "        \"Cao Th·∫Øng\",\"ƒê√†o Duy T·ª´\",\"B√πi Vi·ªán\",\"Tr·∫ßn B√¨nh Tr·ªçng\"\n",
    "    ],\n",
    "    \"Q. Thanh Kh√™\": [\n",
    "        \"ƒêi·ªán Bi√™n Ph·ªß\",\"Th√°i Th·ªã B√¥i\",\"L√™ ƒê·ªô\",\"Nguy·ªÖn Ph∆∞·ªõc Nguy√™n\",\"Tr·∫ßn Cao V√¢n\",\n",
    "        \"Nguy·ªÖn Tri Ph∆∞∆°ng\",\"Tr∆∞ng N·ªØ V∆∞∆°ng\",\"Nguy·ªÖn Ho√†ng\",\"H√† Huy T·∫≠p\",\"H√†m Nghi\",\n",
    "        \"C√π Ch√≠nh Lan\",\"Phan Thanh\",\"Hu·ª≥nh Ng·ªçc Hu·ªá\",\"Tr·∫ßn Xu√¢n L√™\",\"ƒê·ªó Quang\",\n",
    "        \"L√Ω Th√°i T·ªï\",\"Y√™n Kh√™\", \"Ho√†ng Hoa Th√°m\",\"K·ª≥ ƒê·ªìng\",\"V√µ VƒÉn T·∫ßn\",\"Ph·∫°m Nh·ªØ TƒÉng\",\n",
    "        \"Ph·∫°m VƒÉn Ngh·ªã\",\"L√™ Duy ƒê√¨nh\",\"Nguy·ªÖn VƒÉn Hu·ªÅ\",\"Nguy·ªÖn Ch√°nh\"\n",
    "    ],\n",
    "    \"Q. S∆°n Tr√†\": [\n",
    "        \"Tr·∫ßn H∆∞ng ƒê·∫°o\",\"Y·∫øt Ki√™u\",\"Ng√¥ Quy·ªÅn\",\"Chu Huy M√¢n\",\"H·ªì Nghinh\",\n",
    "        \"Ho√†ng Sa\",\"V√µ VƒÉn Ki·ªát\",\"Nguy·ªÖn C√¥ng Tr·ª©\",\"D∆∞∆°ng V√¢n Nga\",\"L√™ T·∫•n Trung\",\n",
    "        \"Tr·∫ßn Nh√¢n T√¥ng\",\"Ph·∫°m C·ª± L∆∞·ª£ng\",\"Ph·∫°m VƒÉn ƒê·ªìng\",\"Gi√°p VƒÉn C∆∞∆°ng\",\n",
    "        \"An ƒê·ªìn\",\"M√¢n Quang\",\"L√™ H·ªØu Tr√°c\",\"Ho√†ng ƒê·ª©c L∆∞∆°ng\",\n",
    "        \"Ng·ªçc H√¢n\",\"Phan K·∫ø B√≠nh\",\"Ph√πng Ch√≠ Ki√™n\"\n",
    "    ],\n",
    "    \"Q. Ng≈© H√†nh S∆°n\": [\n",
    "        \"L√™ VƒÉn Hi·∫øn\",\"Nguy·ªÖn VƒÉn Tho·∫°i\",\"Tr·∫ßn ƒê·∫°i Nghƒ©a\",\"Minh M·∫°ng\",\n",
    "        \"L√™ Quang ƒê·∫°o\",\"H·ªì Xu√¢n H∆∞∆°ng\",\"Nghi√™m Xu√¢n Y√™m\",\"V√µ Ch√≠ C√¥ng\",\n",
    "        \"Tr∆∞·ªùng Sa\",\"Ng≈© H√†nh S∆°n\",\"V√µ Nguy√™n Gi√°p\", \"L·∫°c Long Qu√¢n\",\"Tri·ªáu Vi·ªát V∆∞∆°ng\",\"H·∫£i S∆°n\",\"Phan Huy √în\",\n",
    "        \"Mai An\",\"Th·∫ø L·ªØ\",\"V≈© Qu·ª≥nh\"\n",
    "    ],\n",
    "    \"Q. Li√™n Chi·ªÉu\": [\n",
    "        \"Nguy·ªÖn L∆∞∆°ng B·∫±ng\",\"T√¥n ƒê·ª©c Th·∫Øng\",\"Ho√†ng VƒÉn Th√°i\",\"Nguy·ªÖn T·∫•t Th√†nh\",\n",
    "        \"Kinh D∆∞∆°ng V∆∞∆°ng\",\"Nam Cao\",\"B·∫Øc ƒê·∫©u\",\"B√†u Tr√†m\",\n",
    "        \"Ph·∫°m Nh∆∞ X∆∞∆°ng\",\"√Çu C∆°\",\"L√™ Ng√¥ C√°t\",\"L√™ VƒÉn Long\",\n",
    "        \"Ph·∫°m Khu Ti·∫øt\",\"Nguy·ªÖn ƒê√¨nh T·ª±u\", \"Ho√†ng TƒÉng B√≠\",\"Nguy·ªÖn VƒÉn C·ª´\",\"Ung VƒÉn Khi√™m\",\"L√™ Thi·ªát\",\n",
    "        \"Nguy·ªÖn C·∫£nh D·ªã\",\"T√¥ Hi·ªáu\",\"VƒÉn Cao\",\"Nguy·ªÖn Huy T∆∞·ªüng\"\n",
    "    ],\n",
    "    \"Q. C·∫©m L·ªá\": [\n",
    "        \"T√¥n ƒê·∫£n\",\"Tr·∫ßn Nam Trung\",\"Ph·∫°m H·ªØu K√≠nh\",\"C√°ch M·∫°ng Th√°ng 8\",\"Ti·ªÉu La\",\n",
    "        \"Tr∆∞·ªùng Chinh\",\"ƒêo√†n Nh·ªØ H√†i\",\"ƒêinh Ti√™n Ho√†ng\",\"ThƒÉng Long\",\"Ph·∫°m Ph√∫ Th·ª©\", \"L√™ Tr·ªçng T·∫•n\",\"T√¥n Th·∫•t T√πng\",\"√îng √çch ƒê∆∞·ªùng\",\"ƒê·ªó Huy Uy·ªÉn\",\n",
    "        \"Nguy·ªÖn Nghi√™m\",\"Tr∆∞·ªùng S∆°n\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# (tu·ª≥ ch·ªçn) alias ch√≠nh t·∫£\n",
    "WARD_ALIAS = {\n",
    "    norm_key(\"Hoa Khe\"): \"H√≤a Kh√™\",\n",
    "}\n",
    "STREET_ALIAS = {\n",
    "    norm_key(\"Trung N·ªØ V∆∞∆°ng\"): \"Tr∆∞ng N·ªØ V∆∞∆°ng\",\n",
    "    norm_key(\"Duong Nguyen Van Linh\"): \"Nguy·ªÖn VƒÉn Linh\",\n",
    "}\n",
    "\n",
    "# ==== 2) Sinh map d√πng cho matching ====\n",
    "# District map (qu·∫≠n/huy·ªán/t·ªânh l√¢n c·∫≠n ƒë·ªÉ nh·∫≠n di·ªán n·∫øu ng∆∞·ªùi d√πng ƒë√£ ghi s·∫µn)\n",
    "districts_dn = [\"H·∫£i Ch√¢u\",\"Thanh Kh√™\",\"S∆°n Tr√†\",\"Ng≈© H√†nh S∆°n\",\"Li√™n Chi·ªÉu\",\"C·∫©m L·ªá\",\"H√≤a Vang\",\"Ho√†ng Sa\"]\n",
    "districts_qn = [\"Qu·∫ø S∆°n\",\"ƒêi·ªán B√†n\",\"Duy Xuy√™n\",\"ThƒÉng B√¨nh\",\"N√∫i Th√†nh\"]\n",
    "\n",
    "district_map = { norm_key(d): f\"Q. {d}\" for d in districts_dn if d not in [\"H√≤a Vang\",\"Ho√†ng Sa\"] }\n",
    "district_map.update({ norm_key(\"H√≤a Vang\"): \"H. H√≤a Vang\", norm_key(\"Ho√†ng Sa\"): \"H. Ho√†ng Sa\" })\n",
    "district_map.update({ norm_key(d): f\"H. {d}\" for d in districts_qn })\n",
    "\n",
    "# Ward maps\n",
    "ward_map, ward_district_map = {}, {}\n",
    "for dist, names in WARDS_BY_DISTRICT.items():\n",
    "    for raw in names:\n",
    "        name = WARD_ALIAS.get(norm_key(raw), raw)\n",
    "        k = norm_key(name)\n",
    "        ward_map[k] = f\"P. {name}\"\n",
    "        ward_district_map[k] = dist\n",
    "\n",
    "# Street maps\n",
    "street_map, street_district_map = {}, {}\n",
    "for dist, names in STREETS_BY_DISTRICT.items():\n",
    "    for raw in names:\n",
    "        name = STREET_ALIAS.get(norm_key(raw), clean_street_name(raw))\n",
    "        k = norm_key(name)\n",
    "        street_map[k] = f\"ƒê∆∞·ªùng {name}\"\n",
    "        street_district_map.setdefault(k, dist)\n",
    "\n",
    "# T√™n TP vi·∫øt t·∫Øt\n",
    "city_map = {\n",
    "    norm_key(\"ƒê√† N·∫µng\"): \"ƒê√† N·∫µng\",\n",
    "    norm_key(\"DN\"): \"ƒê√† N·∫µng\",\n",
    "    norm_key(\"Da Nang\"): \"ƒê√† N·∫µng\",\n",
    "    norm_key(\"Qu·∫£ng Nam\"): \"Qu·∫£ng Nam\",\n",
    "    norm_key(\"QN\"): \"Qu·∫£ng Nam\",\n",
    "}\n",
    "\n",
    "# ==== 3) Chu·∫©n ho√° 1 ƒë·ªãa ch·ªâ ====\n",
    "def normalize_address(raw: str, cutoff=0.86):\n",
    "    if pd.isna(raw) or str(raw).strip()==\"\":\n",
    "        return {\"address_clean\": None, \"street\":None,\"ward\":None,\"district\":None,\"city\":None,\"status\":\"empty\"}\n",
    "\n",
    "    s = str(raw)\n",
    "    s = unicodedata.normalize('NFC', s).replace('\\u00A0',' ')\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    key = norm_key(s)\n",
    "\n",
    "    # Nh·∫≠n di·ªán TP nhanh\n",
    "    city = None\n",
    "    for k, canon in city_map.items():\n",
    "        if k in key:\n",
    "            city = canon\n",
    "            break\n",
    "    if city is None and re.search(r'(,|\\s)\\b(dn|ƒën)\\b$', key):\n",
    "        city = \"ƒê√† N·∫µng\"\n",
    "\n",
    "    # N-grams 1..3\n",
    "    tokens = key.split()\n",
    "    ngrams = set(' '.join(tokens[i:i+n]) for n in (1,2,3) for i in range(len(tokens)-n+1))\n",
    "\n",
    "    # Fuzzy match\n",
    "    w_best = max((best_match(g, ward_map, cutoff)     for g in ngrams), key=lambda x: x[1], default=(None,0))\n",
    "    d_best = max((best_match(g, district_map, cutoff) for g in ngrams), key=lambda x: x[1], default=(None,0))\n",
    "    s_best = max((best_match(g, street_map, cutoff)   for g in ngrams), key=lambda x: x[1], default=(None,0))\n",
    "    ward, w_score = w_best\n",
    "    district, d_score = d_best\n",
    "    street, s_score = s_best\n",
    "\n",
    "    # Suy lu·∫≠n t·ª´ ph∆∞·ªùng\n",
    "    if city is None and ward:\n",
    "        city = DEFAULT_CITY\n",
    "    if district is None and ward:\n",
    "        if not hasattr(normalize_address, \"_ward_key_by_label\"):\n",
    "            normalize_address._ward_key_by_label = {v: k for k, v in ward_map.items()}\n",
    "        wkey = normalize_address._ward_key_by_label.get(ward)\n",
    "        if wkey in ward_district_map:\n",
    "            district = ward_district_map[wkey]\n",
    "\n",
    "    # Suy lu·∫≠n t·ª´ ƒë∆∞·ªùng\n",
    "    if city is None and street:\n",
    "        city = DEFAULT_CITY\n",
    "    if district is None and street:\n",
    "        if not hasattr(normalize_address, \"_street_key_by_label\"):\n",
    "            normalize_address._street_key_by_label = {v: k for k, v in street_map.items()}\n",
    "        skey = normalize_address._street_key_by_label.get(street)\n",
    "        if skey in street_district_map:\n",
    "            district = street_district_map[skey]\n",
    "\n",
    "    # Suy city t·ª´ lo·∫°i district\n",
    "    if city is None:\n",
    "        if district and district.startswith(\"Q.\"):\n",
    "            city = \"ƒê√† N·∫µng\"\n",
    "        elif district and district.startswith(\"H. Qu·∫ø S∆°n\"):\n",
    "            city = \"Qu·∫£ng Nam\"\n",
    "\n",
    "    parts = [p for p in [street, ward, district, city] if p]\n",
    "    addr_clean = ', '.join(parts) if parts else s.title()\n",
    "\n",
    "    status = \"ok\"\n",
    "    flags = []\n",
    "    if s_score < cutoff and street:   flags.append(\"street_low_conf\")\n",
    "    if w_score < cutoff and ward:     flags.append(\"ward_low_conf\")\n",
    "    if d_score < cutoff and district: flags.append(\"district_low_conf\")\n",
    "    if not (street or ward or district or city): flags.append(\"no_match\")\n",
    "    if flags: status = \";\".join(flags)\n",
    "\n",
    "    return {\"address_clean\": addr_clean, \"street\": street, \"ward\": ward,\n",
    "            \"district\": district, \"city\": city, \"status\": status}\n",
    "\n",
    "# ==== 4) √ÅP D·ª§NG & GHI FILE ====\n",
    "sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "if src_sheet not in sheets:\n",
    "    raise KeyError(f\"Kh√¥ng t√¨m th·∫•y sheet '{src_sheet}'.\")\n",
    "df_cus = sheets[src_sheet].copy()\n",
    "\n",
    "if addr_col not in df_cus.columns:\n",
    "    raise KeyError(f\"Sheet '{src_sheet}' kh√¥ng c√≥ c·ªôt '{addr_col}'. Columns: {list(df_cus.columns)}\")\n",
    "\n",
    "res = df_cus[addr_col].apply(normalize_address).apply(pd.Series)\n",
    "df_cus[addr_col] = res[\"address_clean\"]\n",
    "# df_cus[\"status\"] = res[\"status\"]   # m·ªü n·∫øu mu·ªën so√°t l·ªói\n",
    "\n",
    "# Xo√° c·ªôt ph·ª• n·∫øu t·ª´ng t·ªìn t·∫°i\n",
    "for c in [\"address_clean\",\"street\",\"ward\",\"district\",\"city\"]:\n",
    "    if c in df_cus.columns:\n",
    "        df_cus.drop(columns=[c], inplace=True)\n",
    "\n",
    "sheets[src_sheet] = df_cus\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as w:\n",
    "    for name, dfi in sheets.items():\n",
    "        dfi.to_excel(w, sheet_name=name, index=False)\n",
    "\n",
    "print(\"Done: ƒë√£ ghi ƒë√® c·ªôt 'address' trong sheet 'customer'.\")\n",
    "print(df_cus[[addr_col]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
